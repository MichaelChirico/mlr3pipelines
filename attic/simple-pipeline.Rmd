---
title: "Showcase: A first Pipeline"
author: "Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


This vignette showcases the general syntax and semantic behind `mlr3pipelines`.

## Simple preprocessing

We first create an mlr task that, which we want to transform with the pipeline.

```{r}
  library(mlr3)
  task = mlr_tasks$get("iris")
```


An often-applied preprocessing step is to simply **center** and **scale** the data to mean $0$ and standard deviation $1$. In order to do this we create a new `PipeOp`: **PipeOpScale**.

```{r}
  op1 = PipeOpScale$new()
```

In order to not leak data from the test set into the training set it is imperative to treat train and test data correctly. For this we require a 
`train` function  that learns the appropriate transformations from the training set and a `test` function that applies the transformation on future data.
This is already handled in `PipeOpScale`:
For further info, we can look into the `$train_dt` and `$test_dt` methods defined in the PipeOp.

```{r}
  op1$train_dt
```

What does `train_dt` do?
  1. It expects a single input `dt`.
  2. It converts the input to a matrix and scales it using `scale`.
  3. It saves the applied transformations (mean and standard 
     deviation of each row) to `self$state`.
  4. It returns the scaled data `sc`.


```{r}
  op1$predict_dt
```

What does `predict_dt` function do?
  1. It expects a single input `dt`.
  2. It converts the transforms the input using the transformations 
     from `self$state`.
  3. It returns the scaled data.


**We can now apply this to our dataset**

```{r}
# Randomly assign 50 Observations to the validation set
task$set_row_role(sample(seq_len(150), 50), "validation")

# And train
scaled = op1$train(list(task))
scaled[[1]]$head()
```

**And use the predict method to rescale with the learned values**

```{r}
# Get validation data
task$row_roles = setNames(task$row_roles, c("validation", "use"))

rescaled = op1$predict(list(task))
rescaled[[1]]$head()
```


## A more complex example: Scale -> PCA -> Learner:

In this example, we first `scale` the data, apply `PCA` and then train a
**Decision Tree** on the transformed learner.

```{r}
  # Create the PipeOp's:
  op1 = PipeOpScale$new()
  op2 = PipeOpPCA$new()
  op3 = PipeOpLearner$new(learner = mlr_learners$get("classif.rpart"))
  # Connect the operators
  root = GraphNode$new(op1)
  root$
    set_next(GraphNode$new(op2))$
    set_next(GraphNode$new(op3))
  # And instantiate the graph
  g = Graph$new(root)
  print(g)
```

## Creating a custom PipeOp

Creating a `PipeOp` requires the following objects:
- An `id` for the new `PipeOp`
- A `train()` function that stores values required for the transformation and
  saves the transformed data.
- A `predict()` function that uses the values obtained in `train()`to
  transform new data.
- Optionally, a `ParamSet` that contains hyperparameter ranges.


In this example we will create a `PipeOp` that allows us to transform our data using **PCA**.

As an `id` we will use `PipeOpPCA`.

In the next step we will define the `train` function:


```{r}
    train = function(inputs) {
      # We require our inputs to be a (list of) Task(s)
      assert_list(inputs, len = 1L, type = "Task")
      task = inputs[[1L]]

      # We obtain data and the feature names from the task
      fn = task$feature_names
      dt = task$data()

      # And then extract principal components.
      # We will focus on the self$param_vals further down.
      pcr = prcomp(as.matrix(dt),
        center = self$param_vals$center,
        scale. = self$param_vals$scale.,
        rank.  = self$param_vals$rank.)
      # Then we save the "pca-model" in the .params slot.
      private$.params = pcr
      # And convert the transformed data to a data.table
      x = as.data.table(pcr$x)

      # Now we drop the old features from the data.table
      dt[, (fn) := NULL]
      # And add the rotated features
      dt[, (colnames(x)) := x]

      # Now we overwrite the data.table in the task
      # with our new data, save it to the
      # "-result" slot
      private$.result = task$overwrite(dt)
      # and return the new task
      private$.result
    }
```

And the `predict()` function:

```{r}
    predict = function() {
      assert_list(self$inputs, len = 1L, type = "Task")
      task = self$inputs[[1L]]
      fn = task$feature_names
      d = task$data()

      # Call train_dt function on features
      rotated = predict(private$.params, as.matrix(d[, ..fn]))
      dt = as.data.table(rotated)

      # Drop old features, add new features
      d[, (fn) := NULL]
      d[, (colnames(dt)) := dt]

      # See "train()"
      private$.result = task$overwrite(d)
      return(private$.result)
    }
```

And the `ParamSet`:

```{r}
      ps = ParamSet$new(params = list(
        ParamFlag$new("center", default = TRUE),
        ParamFlag$new("scale", default = TRUE)
      ))
```

Afterwards we can put everything together and create a new **R6** Object:


```{r}
PipeOpPCA = R6Class("PipeOpPCA",
  inherit = PipeOp,
  public = list(
    initialize = function(id = "pca") {
      super$initialize(id, ps)
    },
    train = train,
    predict2 = predict
  )
)
```
