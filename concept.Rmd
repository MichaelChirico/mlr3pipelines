# Concept

## Executive summary

This design document describes some classes that represent the current API for mlr3pipelines.
In short, a **Pipeline** is a doubly connected graph, that contains several **GraphNode**s.
Each **GraphNode** contains a **PipeOP**, that contains the logic used to manipulate the inputs, i.e.
functions that transform a pipe's inputs to its outputs.

------------------------------------------------------------------------------------------
## class PipeOp

Description:
A PipeOp is a single tranformation of inputs into outputs.
During training it takes inputs, tranforms them, while doing that learns and stores its
parameters and then returns the output.
During prediction it applies the learned params to the input and transforms the input to an output.

A PipeOp specifies the types of inputs and outputs as `intype` and `outtype`, a list of <something specifying types>.
The length of these lists determines the length of input / output the PipeOp produces. Typically the PipeOp input / output
is a list of specified length, but PipeOps with input / output length 1 can specify that they don't use lists but use singular
values instead (`.takeslist` / `.returnslist` set to `FALSE`)

Usage:
- new(id, params) [character(1), ParamSet] -> [PipeOp]
- id  [character(1)]
- param_set [ParamSet]
- par_vals [named list]
- params [any]
- result [any]
- train(input) : [any] -> [any]
- predict(input) : [any] -> [any]
- packages [character]
- `.intype`: [list of something]
- `.outtype`: [list of something]
- `.takeslist`: [logical(1)]
- `.returnslist`: [logical(1)]

Suggestions:
- rename 'params' to 'state'. I mean, come on.

Details:
  `id`: AB that allows to return and set the id of the PipeOps. Ids are user-configurable, and ids of PipeOps in graphs must be unique.
  `param_set`: The set of all exposed parameters of the PipeOp.
  `par_vals`: A named list of parameter settings where all setting must come from `param_set`.
  `params`: The object of learned parameters, obtained in the training step, and applied in the
    predict step. #FIXME: what if the this is NULL? then the PO is untrained! we need a special "empty" object for POs which do not learn anything
  `new`: Baseclass constructor, called by derived classes.
  `train`: Function that is responsible to train on `input`, transform it to output and store the learned `params`.
    If the PipeOp is already trained, already present `params` are overwritten.
  `predict`: Function that is responsible to predict on `input`, and transform it to output by applying the learned `params`.
    If `is_trained = FALSE` the function cannot be applied.
  `result`: A slot to store the result of either the `train` or the `predict` step, after it was
    applied.
  # FIXME: this is a bit weird. Is this set by the pipeop? by the training of the graph? when do we delete this?
  `is_trained`: Is the PipeOp currently trained?
  `.intype`: list of input types to accept
  `.outtype`: list of output types that are returned
  `.takeslist`: `TRUE` if input of `train` / `predict` is a list, `FALSE` if it is a singular value. If this is `FALSE`, `length(.intype)` must be 1
  `.returnslist`: `TRUE` if output of `train` / `predict` is a list, `FALSE` if it is a singular value. If this is `FALSE`, `length(.outtype)` must be 1

## class NodeChannel

Description
Represents an input / output slot of a node. A node can have multiple of these slots for input / output, they may be named and / or numbered.

Usage:
- `name`: [numeric(1) | character(1)]
- `node`: [GraphNode]
- `direction`: [character(1)] `"in"` or `"out"`

Details:
- `name`: Indexes the connection. May be a `character(1)` or a `numeric(1)`.
- `node`: The GraphNode whose input / output is considered.
- `direction` must be `"in"` if this is a channel representing input values of a `PipeOp` or `"out"` if this represents an item of the output list given by that `PipeOp`.

## class GraphNode

Description:
A GraphNode is a node of a (doubly connected) directed acyclic multigraph, where each node carries as payload a single PipeOp,
and each edge represents the flow of a single list-element of a list returned by one PipeOp, into a single list-element
of a list used as input of another PipeOp.
Computational results in that graph only flow forward, but we store connections to predecessors of a node for
convenience. A GraphNode is always a member of exactly one Graph, an can not have connections to GraphNodes
not in this graph.

A GraphNode has a set of "input channels" and "output channels"; edges go from output channels of one node to the input channel of another node. 

Usage:
- `graph` [Graph]
- `pipeop`  [PipeOp]
- `in_channels` [list of NodeChannel]
- `out_channels` [list of NodeChannel]
- `next_node_channels` [list of NodeChannel]
- `prev_node_channels` [list of NodeChannel]
- `next_nodes` [list of GraphNode]: 
- `prev_nodes` [list of GraphNode]: 
- `input_complete` [logical(1)] 
- `output_complete` [logical(1)] 
- `intype` [list of any]
- `outtype` [list of any]

Details:
`graph`: the graph that this node belongs to
`pipeop`: Operator in that node
`in_channels`: list of NodeChannel that can be put into `next_node_channels` of another node to form a connection.
`out_channels`: list of NodeChannel that can be put into `prev_node_channels` of another node to form a connection.
`next_node_channels`: the respective `in_channels` of successor nodes where data flows. Mutable to change connections.
`prev_node_channels`: the respective `out_channels` of predecessor nodes where data comes from. Mutable to change connections.
`next_nodes`: nodes connected by `next_node_channels`, readonly
`prev_nodes`: nodes connected by `prev_node_channels`, readonly
`input_complete`: whether all input channels have a connected node
`output_complete`: whether all output channels have a connected note
`intype` mirrors the pipeop `intype`
`outtype` mirrors the pipeop `outtype`

## class Graph

Description:
The graph is a container class for the complete computational graph. It is made up of a list of
(connected) GraphNodes, it can be trained and predicted.

Usage:
- `new` [Graph]
- `node_list` [list of GraphNode]
- `sorted_node_list` [list of GraphNode]
- `intype` [list of any]]
- `outtype` [list of any]]
- `in_channels` [list of NodeChannel]
- `out_channels` [list of NodeChannel]
- `source_nodes` [list of GraphNode]
- `sink_nodes` [list of GraphNode]
- `add_node()` [GraphNode | PipeOp] -> [Graph]
- `train()` [any] -> [any]
- `predict()` [any] -> [any]
- `plot()`
- `extend()` [Graph] -> [Graph]
- `map()` [function], [logical] -> [list of any]

- `update_connections()`
- `update_ids()`
- `[[`

Aggregated info
- `param_set` [ParamSet]
- `param_vals` [list]
- `packages` [character]

Remove the following?:
- `is_learnt`
- `rhs`
- `lhs`


Details:
- Get node by `[[id]]`
- `new` with optional Graph argument: copy constructor
- `node_list` list of GraphNode, indexed by ID
- `sorted_node_list` like `node_list`, but ordered by their connections
- `intype`: types of the `in_channels`
- `outtype`: types of the `out_channels`
- `source_nodes`: nodes that have unconnected input channels and therefore act as graph input
- `sink_nodes`: nodes that have unconnected output channels and therefore act as graph output
- `add_node`: Mutates graph by adding a PipeOp or GraphNode. GraphNode calls this automatically on construction, so a user should only call this with a PipeOp.

- `train()`: train on input (e.g. Task), returns processed output (e.g. modified task)
- `predict()`: predict on input (e.g. Data), get processed output (e.g. predictions)
- `plot()`: plot of graph
- `extend()`: Add other graph as disjoint union

- get_pipeop: Returns PipeOp by its id.
- ids: Ids of all PipeOps in the graph.
- lhs: Returns all nodes which have no predecessors
- rhs: Returns all nodes which have no successors


## Operation G >> H

Connection operator, connects 2 partial graphs G and H.

[Graph], [Graph] -> [Graph]
If G or H are PipeOps, they the will leveled up to a Graph, containing a single node.

We check that the ids(G) and ids(H) are disjoint (or throw an error), then
we connect the rhs(G) to the lhs(H) as outlined below.
Case 1-1: If length(rhs(G)) == 1 and length(lhs(H)) == 1, so G has a single sink and H has a single source, we directly connect them.
Case n-1: If length(rhs(G)) == n and length(lhs(H)) == 1, so G has multiple sinks, and H a single source, we connect all nodes in rhs(G) withe the node lhs(H).
Case 1-n: If length(rhs(G)) == 1 and length(lhs(H)) == n, we require that the PipeOp in rhs(G) is a so-called broadcaster, see below. We connect
this broadcaster with all elements in lhs(H).
All other cases: Error.

Operator `>>` will always deep-clone its arguments on call.

## greplicate(G, k): Replicate a graph. Results in a graph
[Graph], integer(1) -> [Graph]
If G is a PipeOp it will be leveled up to a Graph, containing a single node.

Copy the structure of G k-times, make all PipeOP-ids unique by post-fixing them all with
"_rep_1" (for the first copy), "_rep_2" (for the second copy) and so on.

greplicate will always deep-clone its arguments on call.

## Aggregation and Broadcasting PipeOps
PipeOps can have the property "aggregate" or "broadcast".

An aggregating PipeOp will only take an input which is a list. This list is made up of the results
of all of its predecessors, when the PipeOp is used in a graph.
Its specific aggregation functionality handles this list (typically
by somehow combinining the elements to a larger object, or selecting something from the input elements).
Examples of aggregators are PipeOpModelAvg, PipeOpFeatureUnion, PipeOpUnbranch.

A broadcasting PipeOp always returns a list. If the PipeOp is used in a graph,
then its result-list is split up into single elements and these elements are propagated one-per-edge
on the outgoing edges. Note that this implies that the PipeOp always has to return a list of the same
length as its encapsulating GraphNode has number of successors.
Examples of broadcasters are PipeOpCopy, PipeOpBranch, PipeOpChunk

# Topological sorting and layers of a Graph
A DAG is made up of layers of nodes, depending on how "late" in the DAG a node appears, i.e., many how previous
computational steps are needed to be processed until we can finally compute that node. To make this more
explicit: We call the set of all nodes with no incoming edges the set of "sources" of a graph, or the
LHS of a graph. This is layer 0.
Layer 1 is made up of all nodes, whose direct predecessors are only in layer 0.
Layer 2 is made up of all nodes, whose direct predecessors are only in layer 1 or 0.
And so on.
The last layer, which is the set of all nodes without outgoing edges, we call the set of
"sinks" of a graphs or the RHS of the graph.


# Training and predicting on a computational graph
When we train a computational graph, we proceed in topological layers. We initialize by
feeding the input object (a task) into each node in layer 0. We then compute all results
in layer 0, store the respective results on each node. Then we train layer 1. This is possible,
as all nodes in layer 1 only require results from layer 0. Then we train layer 2. This is possible
as all nodes in layer 2 only require results from layer 1 or 0. And so on.
Computing the results of one layer can be done in parallel.
After a finite amount of steps, the RHS of the graph will have been processed. This list
of results of the RHS is the result of the graph computation.

Prediction on a graph works exactly the same. We feed the input object to the LHS, then call the
`predict` on each node, layer by layer, until we have done that also on the RHS.


#FIXME: we need to specifiy the concept of brnaching / multiplexing




