% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PipeOpTextVectorizer.R
\name{mlr_pipeops_text_vectorizer}
\alias{mlr_pipeops_text_vectorizer}
\alias{PipeOpTextVectorizer}
\title{PipeOpTextVectorizer}
\format{
\code{\link{R6Class}} object inheriting from \code{\link{PipeOpTaskPreproc}}/\code{\link{PipeOp}}.
}
\description{
Computes a bag-of-word representation from a (set of) columns.
Columns of type \code{character} are split up into words.
Uses the \code{\link[quanteda:dfm]{quanteda::dfm()}},
\code{\link[quanteda:dfm_trim]{quanteda::dfm_trim()}} from the 'quanteda' package.
TF-IDF computation works similarly to \code{\link[quanteda:dfm_tfidf]{quanteda::dfm_tfidf()}}
but has been adjusted for train/test data split using \code{\link[quanteda:docfreq]{quanteda::docfreq()}}
and \code{\link[quanteda:dfm_weight]{quanteda::dfm_weight()}}

In short:
\itemize{
\item Per default, produces a bag-of-words representation
\item If 'n' is set to values > 1 ngrams are computed
\item If 'df_trim' parameters are set, the bag-of-words is trimmed.
\item If 'scheme' parameters are set, term frequence - inverse document frequency is computed.
}

Parameters specify arguments to quanteda's dfm', 'dfm_trim', 'docfreq' and 'dfm_weight'.
What belongs to what can be obtained from each params \code{tags} where \code{tokenizer} are
arguments passed on to \code{\link[quanteda:dfm]{quanteda::dfm()}}.
Defaults to a bag-of-words representation with token counts as matrix entries.

In order to do \code{tf_idf} weighting, set the 'scheme' parameter to 'inverse*'.

The pipeop works as follows:
\enumerate{
\item Words are tokenized using \code{\link[quanteda:tokens]{quanteda::tokens}}.
\item Ngrams are computed using \code{\link[quanteda:tokens_ngrams]{quanteda::tokens_ngrams}}
\item A document-frequency matrix is computed using \code{\link[quanteda:dfm]{quanteda::dfm}}
\item The document-frequency matrix is trimmed using \code{\link[quanteda:dfm_trim]{quanteda::dfm_trim}}
\item The document-frequency matrix is re-weighted (e.g. tfidf) using \code{\link[quanteda:dfm_tfidf]{quanteda::dfm_tfidf}}
}
}
\section{Construction}{
\preformatted{PipeOpTextVectorizer$new(id = "text_vectorizer", param_vals = list())
}
\itemize{
\item \code{id} :: \code{character(1)}\cr
Identifier of resulting object, default \code{"text_vectorizer"}.
\item \code{param_vals} :: named \code{list}\cr
List of hyperparameter settings, overwriting the hyperparameter settings that would otherwise be set during construction. Default \code{list()}.
}
}

\section{Input and Output Channels}{

Input and output channels are inherited from \code{\link{PipeOpTaskPreproc}}.

The output is the input \code{\link[mlr3:Task]{Task}} with all affected features converted to a bag-of-words
representation.
}

\section{State}{

The \verb{$state} is a list with element 'cols': A vector of extracted columns.
}

\section{Parameters}{

The parameters are the parameters inherited from \code{\link{PipeOpTaskPreproc}}, as well as:
\itemize{
\item \code{language} :: \code{character(1)}\cr
Language to use for stopword filtering. Needs to be either in
\code{stopwords::stopwords_getlanguages("snowball")} or \code{"smart"}.
'smart' coresponds to \code{stopwords::stopwords(source = "smart")}, which
also removes one-character strings. Default: 'smart'.
\item \code{remove_stopwords} :: \code{logical(1)}\cr
Remove stopwords according to 'language'? Default: \code{TRUE}.
\item \code{tolower} :: \code{logical(1)}\cr
Convert to lower case? See \code{\link[quanteda:dfm]{quanteda::dfm}}. Default: \code{TRUE}.
\item \code{stem} :: \code{logical(1)}\cr
Stemming? See \code{\link[quanteda:dfm]{quanteda::dfm}}. Default: \code{FALSE}.
\item \code{what} :: \code{character(1)}\cr
Tokenization splitter. See \code{\link[quanteda:tokens]{quanteda::tokens}}. Default: \code{word}.
\item \code{remove_punct} :: \code{logical(1)}\cr
See \code{\link[quanteda:tokens]{quanteda::tokens}}. Default: \code{FALSE}.
\item \code{remove_url} :: \code{logical(1)}\cr
See \code{\link[quanteda:tokens]{quanteda::tokens}}. Default: \code{FALSE}.
\item \code{remove_symbols} :: \code{logical(1)}\cr
See \code{\link[quanteda:tokens]{quanteda::tokens}}. Default: \code{FALSE}.
\item \code{remove_numbers} :: \code{logical(1)}\cr
See \code{\link[quanteda:tokens]{quanteda::tokens}}. Default: \code{FALSE}.
\item \code{remove_separators} :: \code{logical(1)}\cr
See \code{\link[quanteda:tokens]{quanteda::tokens}}. Default: \code{TRUE}.
\item \code{split_hypens} :: \code{logical(1)}\cr
See \code{\link[quanteda:tokens]{quanteda::tokens}}. Default: \code{FALSE}.
\item \code{n} :: \code{integer}\cr
Vector of ngram lengths. See \code{\link[quanteda:tokens_ngrams]{quanteda::tokens_ngrams}}. Default: 1.
\item \code{skip} :: \code{integer}\cr
Vector of skips. See \code{\link[quanteda:tokens_ngrams]{quanteda::tokens_ngrams}}. Default: 0.
\item \code{sparsity} :: \code{numeric(1)}\cr

Desired sparsity of the 'tfm' matrix. See \code{\link[quanteda:dfm_trim]{quanteda::dfm_trim}}. Default: \code{NULL}.
\item \code{max_termfreq} :: \code{numeric(1)}\cr
Maximum term frequency in the 'tfm' matrix. See \code{\link[quanteda:dfm_trim]{quanteda::dfm_trim}}. Default: \code{NULL}.
\item \code{min_termfreq} :: \code{numeric(1)}\cr
Minimum term frequency in the 'tfm' matrix. See \code{\link[quanteda:dfm_trim]{quanteda::dfm_trim}}. Default: \code{NULL}.
\item \code{termfreq_type} :: \code{character(1)}\cr
How to asess term frequency. See \code{\link[quanteda:dfm_trim]{quanteda::dfm_trim}}. Default: 'count'.
\item \code{scheme_tf} :: \code{character(1)} \cr
Weighting scheme for term frequency: See \code{\link[quanteda:dfm_weight]{quanteda::dfm_weight}}. Default: 'count'.
\item \code{scheme} :: \code{character(1)} \cr
Weighting scheme for document frequency: See \code{\link[quanteda:docfreq]{quanteda::docfreq}}. Default: 'unary' (1 for each document).
\item \code{smoothing} :: \code{numeric(1)}\cr
See \code{\link[quanteda:docfreq]{quanteda::docfreq}}. Default: 0.
\item \code{k} :: \code{numeric(1)}\cr
See \code{\link[quanteda:docfreq]{quanteda::docfreq}}. Default: 0.
\item \code{threshold} :: \code{numeric(1)}\cr
See \code{\link[quanteda:docfreq]{quanteda::docfreq}}. Default: 0.
\item \code{base} :: \code{numeric(1)}\cr
See \code{\link[quanteda:docfreq]{quanteda::docfreq}}. Default: 10.
}
}

\section{Internals}{

See Description. Internally uses the 'quanteda' package.
All columns selected via \code{affect_columns} are concatenated before computing the bag of words.
Tokens not seen during training are silently dropped.
}

\section{Methods}{

Only methods inherited from \code{\link{PipeOpTaskPreproc}}/\code{\link{PipeOp}}.
}

\examples{
library("mlr3")
library("data.table")
# create some text data
dt = data.table(
  txt = replicate(150, paste0(sample(letters, 3), collapse = " "))
)
task = tsk("iris")$cbind(dt)

pos = po("text_vectorizer", param_vals = list(language = "en"))

pos$train(list(task))[[1]]$data()

one_line_of_iris = task$filter(13)

one_line_of_iris$data()

pos$predict(list(one_line_of_iris))[[1]]$data()
}
\seealso{
Other PipeOps: 
\code{\link{PipeOpEnsemble}},
\code{\link{PipeOpImpute}},
\code{\link{PipeOpProxy}},
\code{\link{PipeOpTaskPreproc}},
\code{\link{PipeOp}},
\code{\link{mlr_pipeops_boxcox}},
\code{\link{mlr_pipeops_branch}},
\code{\link{mlr_pipeops_chunk}},
\code{\link{mlr_pipeops_classbalancing}},
\code{\link{mlr_pipeops_classifavg}},
\code{\link{mlr_pipeops_classweights}},
\code{\link{mlr_pipeops_colapply}},
\code{\link{mlr_pipeops_collapsefactors}},
\code{\link{mlr_pipeops_copy}},
\code{\link{mlr_pipeops_datefeatures}},
\code{\link{mlr_pipeops_encodeimpact}},
\code{\link{mlr_pipeops_encodelmer}},
\code{\link{mlr_pipeops_encode}},
\code{\link{mlr_pipeops_featureunion}},
\code{\link{mlr_pipeops_filter}},
\code{\link{mlr_pipeops_fixfactors}},
\code{\link{mlr_pipeops_histbin}},
\code{\link{mlr_pipeops_ica}},
\code{\link{mlr_pipeops_imputehist}},
\code{\link{mlr_pipeops_imputemean}},
\code{\link{mlr_pipeops_imputemedian}},
\code{\link{mlr_pipeops_imputemode}},
\code{\link{mlr_pipeops_imputenewlvl}},
\code{\link{mlr_pipeops_imputesample}},
\code{\link{mlr_pipeops_kernelpca}},
\code{\link{mlr_pipeops_learner}},
\code{\link{mlr_pipeops_missind}},
\code{\link{mlr_pipeops_modelmatrix}},
\code{\link{mlr_pipeops_mutate}},
\code{\link{mlr_pipeops_nop}},
\code{\link{mlr_pipeops_pca}},
\code{\link{mlr_pipeops_quantilebin}},
\code{\link{mlr_pipeops_regravg}},
\code{\link{mlr_pipeops_removeconstants}},
\code{\link{mlr_pipeops_scalemaxabs}},
\code{\link{mlr_pipeops_scalerange}},
\code{\link{mlr_pipeops_scale}},
\code{\link{mlr_pipeops_select}},
\code{\link{mlr_pipeops_smote}},
\code{\link{mlr_pipeops_spatialsign}},
\code{\link{mlr_pipeops_subsample}},
\code{\link{mlr_pipeops_threshold}},
\code{\link{mlr_pipeops_unbranch}},
\code{\link{mlr_pipeops_yeojohnson}},
\code{\link{mlr_pipeops}}
}
\concept{PipeOps}
