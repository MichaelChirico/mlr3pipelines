---
title: "Quick Introduction to mlr3pipelines"
author: "Martin Binder"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Quick Introduction to mlr3pipelines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE,
  collapse = TRUE,
  comment = "#>"
)
set.seed(8008135)
compiler::enableJIT(0)
library("mlr3")
library("mlr3pipelines")
lgr::get_logger("mlr3")$set_threshold("warn")
```

This vignette gives a very quick introduction to `mlr3pipelines` that skips over some details to get you running quickly. This assumes the `mlr3` and `mlr3pipelines` packages are loaded:
```{r, eval = FALSE}
library("mlr3")
library("mlr3pipelines")
```

## The Building Blocks: `PipeOp`s

The building blocks of `mlr3pipelines` are **`PipeOp`**-objects. They can be constructed directly using `PipeOp<NAME>$new()`, but the recommended way is to retrieve them from the **`mlr_pipeops`** dictionary. See `as.data.table(mlr_pipeops)` for a list and `?mlr_pipeops` for more details.

```{r}
pca = mlr_pipeops$get("pca")
```

Some pipeops require additional arguments for construction. Hyperparameters can also be set through the `param_vals` argument.
```{r}
filter = mlr_pipeops$get("filter",
  filter = mlr3featsel::FilterVariance$new(),
  param_vals = list(filter.frac = 0.5))
```

## `Graph`s

`PipeOp`s are combined into **`Graph`**s. The hard way is to construct an empty `Graph` first, fill it with `PipeOp`s, and then to add connecting edges between them. `PipeOp`s are identified by their **`$id`** when doing this. Note that the operations all modify the object in-place and return the object itself, so multiple modifications can be chained.
```{r}
graph = Graph$new()$
  add_pipeop(pca)$
  add_pipeop(filter)$
  add_edge("variance", "pca")  # add connection filter (id "variance") -> pca
```

The much quicker way is to use the **`%>>%`**-operator to chain `PipeOp`s or `Graph`s. The same result as above can be achieved by doing
```{r}
graph = pca %>>% filter
```

Inspect the `Graph` using its **`$plot()`** function:
```{r}
graph$plot()
```

If adding multiple `PipeOp`s of the same kind it becomes necessary to change the id to avoid name clashes; either by accessing the `$id` slot or during construction.
```{r, error = TRUE}
graph$add_pipeop(mlr_pipeops$get("pca"))
```
```{r}
graph$add_pipeop(mlr_pipeops$get("pca", id = "pca2"))
```

## Preprocessing with `Graph`s

Graphs can be used for processing data. They have a **`$train()`** function that performs the `$train()` operations of all constituent `PipeOp`s along the `Graph`'s edges, and an equivalent **`$predict()`** function. `$train()` is used on data both for processing and to build an internal model of the data (which can be inspected using `graph$pipeops$<PipeOp Name>$state`). This model is used by the `$predict()` functions. E.g. when doing feature filtering during model training, it is necessary to remove the same features from new prediction data. Most `PipeOp`s handle data in the `mlr3` `Task` format, so the argument to `$train()` and `$predict()` is often a `Task`. The output is always a list with as many elements as there are terminal nodes in the `Graph` (usually just one).

```{r}
task = mlr_tasks$get("iris")  # the "iris" task
task_processed = graph$train(task)[[1]]
```

## Model Fitting with `Graph`s

The most fitting (get it?) use of `Graph`s is to build combined preprocessing and model fitting pipelines that can be used as `mlr3` `Learner`s. For this it is necessary to use two complementary building blocks: **`PipeOpLearner`**, which is a processing pipeline that takes `Task`s (during training and prediction) and produces `Prediction`s (during prediction).
```{r}
graph = pca %>>% filter %>>%
  mlr_pipeops$get("learner",
    learner = mlr_learners$get("classif.rpart"))
```
```{r}
graph$train(task)
graph$predict(task$clone(deep = TRUE)$filter(1:3))
```

The dual of `PipeOpLearner` (which takes a `Learner` and puts it inside a `PipeOp` so that it can be inside a `Graph`) is the **`GraphLearner`**, which takes a `Graph` and encapsulates it in a `Learner`. The `Graph` being encapsulated must always produce a `Prediction` with its `$predict()` call, so it will probably contain at least one `PipeOpLearner`.

```{r}
glrn = GraphLearner$new(graph)
```

This learner can be used for model fitting, resampling, benchmarking, and tuning.
```{r}
cv10 = mlr_resamplings$get("cv")
resample(task, glrn, cv10)
```

## Hyperparameters

Individual `PipeOp`s offer hyperparameters as their **`$param_set`** slot that can be read and written from **`$param_set$values`**. This is done using the *[paradox](https://github.com/mlr-org/paradox)* package, see there for more info. The parameters get passed down to the `Graph`, and finally to the `GraphLearner`. This makes it not only possible to easily change change the behaviour of a `Graph` / `GraphLearner` and try different settings manually, but also to perform tuning using the *[mlr3tuning](https://github.com/mlr-org/mlr3tuning)* package.

```{r}
glrn$param_set$values$variance.filter.frac = 0.25
resample(task, glrn, cv10)
```

```{r}
library("paradox")
ps = ParamSet$new(list(
  ParamDbl$new("classif.rpart.cp", lower = 0, upper = 0.05),
  ParamDbl$new("variance.filter.frac", lower = 0.25, upper = 1)
))
```
```{r}
library("mlr3tuning")
fitness = PerformanceEvaluator$new(task, glrn, cv10, "classif.ce", ps)
tuner = TunerRandomSearch$new(fitness, TerminatorEvaluations$new(10))
```

```{r}
tuner$tune()
```

```{r}
tuner$tune_result()
```

## Non-Linear `Graph`s

The `Graph`s seen so far all have a linear structure. Some `PipeOp`s may have multiple input or output channels, usually given during construction from their **`innum`** or **`outnum`** parameter, that make it possible to create non-linear `Graph`s with alternative paths taken by the data.

When using the `%>>%`-operator, it is necessary to always combine `PipeOp`s or `Graph`s with matching number of input / output channels. A quick way of putting different `PipeOp`s/`Graph`s "next to" each other to make them fit with a multi-input/output `Graph` is **`gunion()`**; to create parallel copies of the *same* `PipeOp`/`Graph` use **`greplicate()`**. Use the **`PipeOpNULL`** operator for paths that should not change the data.

### Conditional Execution / Branching

The **`PipeOpBranch`** and **`PipeOpUnbranch`** `PipeOp`s make it possible to specify multiple alternative paths, only one of which is taken by the data. The active path is determined by a hyperparameter. This makes tuning of alternative preprocessing paths (or learner models) possible.

`PipeOp(Un)Branch` is initialized either with the number of branches, or with a `character`-vector indicating the names of the branches. If names are given, the branch-choosing hyperparameter becomes more readable.

```{r}
graph = mlr_pipeops$get("branch", c("null", "pca", "scale")) %>>%
  gunion(list(
      mlr_pipeops$get("null", id = "null1"),
      mlr_pipeops$get("pca"),
      mlr_pipeops$get("scale")
  )) %>>%
  mlr_pipeops$get("unbranch", c("null", "pca", "scale"))
graph$plot()
```

### Concurrent Execution / Copying, with Feature Union or Ensembles

The **`PipeOpCopy`** `PipeOp` sends the same data along multiple paths concurrently, which leads to all `PipeOp`s coming afterwards being executed. The results of all branches are then often collected using **`PipeOpFeatureUnion`** or an ensemble `PipeOp`: **`PipeOpMajorityVote`** (for classification) or **`PipeOpModelAvg`** (for regression). The number of output branches for `PipeOpCopy` must be given during initialization, while the converging `PipeOp`s must be given the number of input streams.

An example of a preprocessing `Graph` that adds the principal components of a task next to the unmodified features of that task:
```{r}
graph = mlr_pipeops$get("copy", outnum = 2) %>>%
  gunion(list(
      mlr_pipeops$get("null"),
      mlr_pipeops$get("pca")
  )) %>>%
  mlr_pipeops$get("featureunion", 2)
graph$plot()
```

An example of simple bagging, using **`PipeOpSubsample`** and `greplicate()`:
```{r}
single_path = mlr_pipeops$get("subsample") %>>%
  mlr_pipeops$get("learner",
    learner = mlr_learners$get("classif.rpart"))

all_paths = greplicate(single_path, n = 5)

graph = all_paths %>>% mlr_pipeops$get("majorityvote", innum = 5)

graph$plot()
```

### Stacking

A powerful machine learning method is "stacking", where the prediction of one method is used as a feature for another method. This can be achieved with the **`PipeOpLearnerCV`** which uses a `Learner` to produce a prediction `Task`, both during training (combining the holdout samples from a cross-validation) and during prediction. The resulting `Task` contains the predictions *only*, so it is often used in combination with `PipeOpCopy`, `PipeOpNULL`, and `PipeOpFeatureUnion`:

```{r}
library("mlr3learners")
graph = mlr_pipeops$get("copy", 3) %>>%
  gunion(list(
      mlr_pipeops$get("null"),
      mlr_pipeops$get("learner_cv", mlr_learners$get("regr.rpart")),
      mlr_pipeops$get("learner_cv", mlr_learners$get("regr.lm"))
  )) %>>%
  mlr_pipeops$get("featureunion", 3) %>>%
  mlr_pipeops$get("learner", mlr_learners$get("regr.glmnet"))
graph$plot()
```

## Feature Overview

* Simple preprocessing operations:
    * Numeric manipulation: *PipeOpScale*, *PipeOpPCA*
    * Imputation: *PipeOpImpute*
    * Feature filtering: *PipeOpFilter*.
    * Data feature type conversion: *PipeOpEncode*.
    * Undersampling / subsampling for speed and outcome class imbalance handling: *PipeOpBalanceSample*, *PipeOpSubsample*
* *mlr3* *Learner* as operation in a *Graph*, both returning a "*Prediction*" (*PipeOpLearner*) and an added data feature for super learning (*PipeOpLearnerCV*).
* Backup prediction for prediction samples that the main `Learner` can not handle for: *PipeOpBackupLearner*
* Simultaneous alternative paths
    * with same input data: *PipeOpCopy*.
    * with chunked input `Task`s: *PipeOpChunk*
* Simple ensemble methods on Predictions: *PipeOpMajorityVote*, *PipeOpModelAvg*.
* Combination of data from alternative paths: *PipeOpFeatureUnion*.
* Optional alternative paths, chosen by hyperparameter: *PipeOpBranch*, *PipeOpUnbranch*.
* Non-Operation: *PipeOpNULL*
