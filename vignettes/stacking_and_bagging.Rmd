---
title: "Showcase: Bagging and Stacking"
author: "Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Leveraging the different operations available to connect `PipeOps` to very powerfull graphs. 
This vignette introduces two such well-known graph structures, that allow us to enhance single learners 
to more powerfull combinations.

```{r}
  library(mlr3)
  library(mlr3pipelines)
  task = mlr_tasks$get("iris")
```

## Bagging

We first examine Bagging introduced by Breimann (1994).
The basic idea is to create multiple predictors and then aggregate those to a single, more powerfull predictor.
> "... multiple versions are formed
> by making bootstrap replicates of the learning set 
> and using these as new learning sets" (Breimann 1994)

Bagging then aggregates a set of predictors by averaging (regression) or majority vote (classification). 
The idea behind bagging is, that a set of weak, but different predictors can be combined in order to arrive 
at a single, better predictor.

We can achieve this by downsampling our data before training a learner, repeating this for say $10$ times and then performing a majority vote on the predictions.

First, we create a simple pipeline, that uses 
`PipeOpDownsample` before a `PipeOpLearner` is trained: 

```{r}
  single_pred = PipeOpDownsample$new() %>>%
    PipeOpLearner$new(mlr_learners$get("classif.rpart"))
```

We can now repeat this $10$ times using `greplicate`.

```{r}
  pred_set = greplicate(single_pred, 10L)   
```

In order to visually inspect the resulting graph, 
we can plot it:

```{r, fig.height = 6}
  pred_set$plot()
```

Afterwards we need to aggregate the 10 pipelines 
to form a single model: 

```{r}
  bagging = pred_set %>>%
    PipeOpMajorityVote$new(innum = 10L)
```

and plot again to see what happens:

```{r, fig.height = 6}
  bagging$plot()
```

This pipeline can again be used in conjunction with
`GraphLearner` in order for Bagging to be used like a 
[mlr3::Learner].

```{r, fig.height = 6}
  baglrn = GraphLearner$new(bagging)
  e = Experiment$new(task = task, learner = baglrn)
  e$train()
  e$predict()
```

## Stacking 

Stacking is another technique that can improve model performance. The basic idea behind stacking is, that 
using predictions from one model as features of a subsequent model can possibly improve performance. 

A very simple possibility would be to train a 
decision tree and use the predictions from this 
model in conjunction with the original features in order to train an additional model on top. 
The basic idea behind this is, that patterns a model detected in the data can be used by a higher level model, and thus result in a better performance.

In order to limit overfitting, we additionally do not predict on
the original predictions of the learner, but instead on out-of-bag predictions. This is automatically done by 
`PipeOpLearnerCV`.

We first create a level 0 learner, which is used to
extract a lower level prediction.
```{r, fig.height = 6}
  lrn = mlr_learners$get("classif.rpart")
  lrn_0 = PipeOpLearnerCV$new(lrn)
```