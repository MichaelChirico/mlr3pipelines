---
title: "Showcase: Custom PipeOps"
author: "Florian Pfisterer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


This vignette showcases how the `mlr3pipelines` package can be extended to include custom `PipeOps`.

In order to test and showcase our PipeOp, we first create a `Task`.
```{r}
  library(mlr3)
  task = mlr_tasks$get("iris")
```

# How Things Work Around Here

`mlr3pipelines` is fundamentally built around [`R6`](https://r6.r-lib.org/). To create custom `PipeOp` objects, it can only help to [familiarize yourself with it](https://adv-r.hadley.nz/r6.html).

In principle, all a `PipeOp` must do is inherit from the `PipeOp` `R6`-class and implement the `train()` and `test()` functions. There are, however, several auxiliary subclasses that can make the creation of *certain* operations much easier.

# General Case: `PipeOpCopy`

A very simple, yet useful, `PipeOp` is `PipeOpCopy`, which takes a single input and creates a variable number of output channels, all of which get a version of the input data. It is a good example that showcases the important parts of the definition of a custom `PipeOp`. We will show a simplified version here, **`PipeOpCopyTwo`**, that creates exactly two copies of its input data.

## First Steps: Inheriting from `PipeOp`

The first part of creating a custom `PipeOp` is inheriting from `PipeOp`. We make a mental note that we need to implement a `train()` and `predict()` function, and that we probably want to have an `initialize()` as well:

```{r, eval = FALSE}
PipeOpCopyTwo <- R6Class("PipeOpCopyTwo",
  inherit = PipeOp,
  public = list(
    initialize = function(id = "copy.two") {
      ....
    },

    train = function(inputs) {
      ....
    },

    predict = function(inputs) {
      ....
    }
  )
)
```

## Channel Definitions

We need to tell the `PipeOp` the layout of its channels: How many there are, what their names are going to be, and what types are acceptable. This is done on initialization of the `PipeOp` (using a `super$initialize` call) by giving the `input` and `output` `data.table` objects. These must have three columns: a `"name"` column giving the names of input and output channels, and a `"train"` and `"predict"` column naming the class of objects we expect during training and prediction as input / output. A special value for these classes is `"*"`, which indicates that any class will be accepted--our simple copy operator accepts any kind of input, so this will be useful. We have only one input, but two output channels.

By convention, we should name a single channel `"input"` or `"output"`, and a group of channels [`"input1"`, `"input2"`, ...], unless there is a reason to give specific different names. Therefore, our `input` `data.table` will have a single row `<"input", "*", "*">`, and our `output` table will have two rows, `<"output1", "*", "*">` and `<"output2", "*", "*">`.

All of this is given to the `PipeOp` creator. Our `initialize()` will thus look as follows:
```{r, eval = FALSE}
    initialize = function(id = "copy.two") {
      input = data.table(name = "input", train = "*", predict = "*")
      # the following will create two rows and automatically fill the `train`
      # and `predict` cols with "*"
      output = data.table(name = c("output1", "output2"),
                          train = "*", predict = "*")
      super$initialize(id,
        input = input,
        output = output
      )
    }
```

## Train and Predict

Both `train()` and `predict()` will receive a `list` as input and must give a `list` in return. According to our `input` and `output` definitions, we will always get a list with a single element as input, and will need to return a list with two elements. Because all we want to do is create two copies, we will just write `c(inputs, inputs)`. Two things to consider:
* The `train()` function must always modify the `self$state` variable to something that is not `NULL` or `NO_OP`. This is because it shows that a `PipeOp`  has been trained on data, even if the state itself is not important to the `PipeOp` (as in our case). Therefore, our `train()` will set `self$state = list()`.
* It is not necessary to "clone" our input or make deep copies, because we don't modify the data. However, if we were changing a reference-passed object, for example by changing data in a `Task`, we would have to make a deep copy first. This is because a `PipeOp` may never modify its input object.

Our `train()` and `predict()` functions are now:
```{r, eval = FALSE}
    train = function(inputs) {
      self$state = list()
      c(inputs, inputs)
    },

    predict = function(inputs) {
      c(inputs, inputs)
    }
```

## Putting it Together

The whole definition thus becomes
```{r}
PipeOpCopyTwo = R6Class("PipeOpCopyTwo",
  inherit = PipeOp,
  public = list(
    initialize = function(id = "copy.two") {
      super$initialize(id,
        input = data.table(name = "input", train = "*", predict = "*"),
        output = data.table(name = c("output1", "output2"),
                            train = "*", predict = "*")
      )
    },

    train = function(inputs) {
      self$state = list()
      c(inputs, inputs)
    },

    predict = function(inputs) {
      c(inputs, inputs)
    }
  )
)
```

We can create an instance of it, put it in a graph, and see what happens when we train it on something:

```{r}
poct = PipeOpCopyTwo$new()
gr = Graph$new()
gr$add_pipeop(poct)

print(gr)

result = gr$train(task)

str(result)
```

# Special Case: Preprocessing

Many PipeOps perform an operation on exactly one `Task`, and return exactly one `Task`. They may even not care about the "Target" / "Outcome" variable of that task, and only do some modification of some input data. However, it is usually important to them that the `Task` on which they perform prediction has the same data columns as the `Task` on which they train. For these cases the auxiliary base class `PipeOpTaskPreproc` exists. It inherits from `PipeOp` itself, and other PipeOps should use it if they fall in the kind of use-case named above.

When inheriting from `PipeOpTaskPreproc`, one must either implement the `train_task` and `predict_task` functions, or the `train_dt` and `predict_dt` functions, depending on whether wants to operate on a `Task` object or on `data.table`s. In the second case, it may optionally also overload the `select_cols` function, which chooses which of the incoming `Task`'s features are given to the `train_dt`  / `predict_dt` functions.

The following will show two examples: `PipeOpDropNA`, which removes a `Task`'s rows with missing values during training (and implements `{train,predict}_task`), and `PipeOpScale`, which scales a `Task`'s numeric columns (and implements `{train,predict}_dt` and `select_cols`).

## `PipeOpDropNA`

Dropping rows with missing values may be important when training a model that can not handle them. 

## `PipeOpScale`

An often-applied preprocessing step is to simply **center** and **scale** the data to mean $0$ and standard deviation $1$. In order to do this we create a new `PipeOp`: **PipeOpScale**.


```{r}
  library(mlr3pipelines)
  op1 = PipeOpScale$new()
```

### Train and Test transformations 

In order to not leak data from the test set into the training set it is imperative to treat train and test data correctly. For this we require a 
`train` function  that learns the appropriate transformations from the training set and a `test` function that applies the transformation on future data.
This is already handled in `PipeOpScale`:
For further info, we can look into the `$train_dt` and `$test_dt` methods defined in the PipeOp.

```{r}
  op1$train_dt
```

What does `train_dt` do?
  1. It expects a single input `dt`.
  2. It converts the input to a matrix and scales it using `scale`.
  3. It saves the applied transformations (mean and standard 
     deviation of each row) to `self$state`.
  4. It returns the scaled data `sc`.


```{r}
  op1$predict_dt
```

What does `predict_dt` do?
  1. It expects a single input `dt`.
  2. It converts the transforms the input using the transformations 
     from `self$state`.
  3. It returns the scaled data.

**We can now apply this to our dataset**

```{r}
# Randomly assign 50 Observations to the validation set
task$set_row_role(sample(seq_len(150), 50), "validation")

# And train
scaled = op1$train(list(task))
scaled[[1]]$head()
```

**And use the predict method to rescale with the learned values**

```{r, eval = FALSE}
# Get validation data
task$row_roles = setNames(task$row_roles, c("validation", "use"))

rescaled = op1$predict(list(task))
rescaled[[1]]$head()
```

### General Semantics

All PipeOps inherit from a base-class **PipeOp** that provides the general 
structure and functionality of each PipeOp.

The following slots (and more) are thus contained in each PipeOp:
- `train`                 :: `function(inputs)`
  Function used to train the PipeOp.
- `predict`               :: `function(inputs)`
  Function used to predict with the PipeOp.
- `id`                         :: [character]
  Allows to return and set the id of the PipeOps. Ids are user-configurable, and ids of PipeOps in graphs must be unique.
- `param_set`                  :: [ParamSet]
  The set of all exposed parameters of the PipeOp.
- `param_vals`                   :: named [list]
  Parameter settings where all setting must come from `param_set`, named with param IDs.
- `state`                      :: any
  The object of learned parameters, obtained in the training step, and applied in the predict step.
- `is_trained`                 :: `logical(1)`
  Is the PipeOp currently trained?


Additional mempers of a PipeOp can be obtained via: 

```{r, eval = FALSE}
ls(op1)
```

### Abstractions

The PipeOp used in the earlier example only manipulates the features of a Task. All other Task properties stay intact. This means, we only have to
manipulate the `data.table` containing the features instead of manipulating
the whole task.
As this is a fairly common operation, an abstract base class `PipeOpDT` 
has been created which facilitates this.
This let's us define a set of functions `train_dt` and `predict_dt`, which 
expect a `data.table` as input and output. All other conversions are handled implicitly by the PipeOp.

The full code for PipeOpScale now looks as follows:

```{r, eval = FALSE}
PipeOpScale = R6Class("PipeOpScale",
  inherit = PipeOpDT,
  public = list(
    initialize = function(id = "scale") {
      ps = ParamSet$new(params = list(
        ParamLgl$new("center", default = TRUE),
        ParamLgl$new("scale", default = TRUE)
      ))
      super$initialize(id, ps)
    },

    train_dt = function(dt) {
      sc = scale(as.matrix(dt),
        center = self$param_vals$center,
        scale = self$param_vals$scale)

      self$state = list(
        center = attr(sc, "scaled:center") %??% 0,
        scale = attr(sc, "scaled:scale") %??% 1
      )
      sc
    },

    predict_dt = function(newdt) {
      t((t(newdt) - self$state$center) / self$state$scale)
    }
  )
)
```

* We create a new `R6` class 'PipeOpScale'.
* This class is a child class of `PipeOpDT`.
* We initialize a new PipeOp by defining a `ParamSet`, i.e. a set of possible
  parameter settings, and using it to initialize the parent class.
* We define functions `train_dt` and `test_dt` that expect a `data.table` as 
  input and return a `data.table`.



## An advanced example: PipeOpPCA Operator

In this example we will create a `PipeOp` that allows us to transform our data using **PCA**.

As an `id` we will use `PipeOpPCA`.

In the next step we will define the `train` function:

```{r}
    train = function(inputs) {
      # We require our inputs to be a (list of) Task(s)
      assert_list(inputs, len = 1L, type = "Task")
      task = inputs[[1L]]

      # We obtain data and the feature names from the task
      fn = task$feature_names
      dt = task$data()

      # And then extract principal components.
      # We will focus on the self$param_vals further down.
      pcr = prcomp(as.matrix(dt),
        center = self$param_vals$center,
        scale. = self$param_vals$scale.,
        rank.  = self$param_vals$rank.)
      # Then we save the "pca-model" in the .params slot.
      private$.params = pcr
      # And convert the transformed data to a data.table
      x = as.data.table(pcr$x)

      # Now we drop the old features from the data.table
      dt[, (fn) := NULL]
      # And add the rotated features
      dt[, (colnames(x)) := x]

      # Now we overwrite the data.table in the task
      # with our new data, save it to the
      # "-result" slot
      private$.result = task$overwrite(dt)
      # and return the new task
      private$.result
    }
```

And the `predict()` function:

```{r}
    predict = function() {
      assert_list(self$inputs, len = 1L, type = "Task")
      task = self$inputs[[1L]]
      fn = task$feature_names
      d = task$data()

      # Call train_dt function on features
      rotated = predict(private$.params, as.matrix(d[, ..fn]))
      dt = as.data.table(rotated)

      # Drop old features, add new features
      d[, (fn) := NULL]
      d[, (colnames(dt)) := dt]

      # See "train()"
      private$.result = task$overwrite(d)
      return(private$.result)
    }
```

And the `ParamSet`:

```{r}
      ps = ParamSet$new(params = list(
        ParamFlag$new("center", default = TRUE),
        ParamFlag$new("scale", default = TRUE)
      ))
```

Afterwards we can put everything together and create a new **R6** Object:


```{r}
PipeOpPCA = R6Class("PipeOpPCA",
  inherit = PipeOp,
  public = list(
    initialize = function(id = "pca") {
      super$initialize(id, ps)
    },
    train = train,
    predict2 = predict
  )
)
```
